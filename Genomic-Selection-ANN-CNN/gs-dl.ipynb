{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-04T13:08:15.726680Z","iopub.execute_input":"2022-05-04T13:08:15.727248Z","iopub.status.idle":"2022-05-04T13:08:15.777483Z","shell.execute_reply.started":"2022-05-04T13:08:15.727129Z","shell.execute_reply":"2022-05-04T13:08:15.775221Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"pip install talos\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-04T13:12:46.226210Z","iopub.execute_input":"2022-05-04T13:12:46.226555Z","iopub.status.idle":"2022-05-04T13:13:37.776411Z","shell.execute_reply.started":"2022-05-04T13:12:46.226521Z","shell.execute_reply":"2022-05-04T13:13:37.774531Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom scipy import stats\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import scale\n\n# keres modules\nfrom keras import regularizers\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.layers import Flatten, Conv1D, MaxPooling1D\nfrom keras.activations import relu, elu, linear, softmax\nfrom keras.callbacks import EarlyStopping, Callback\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom tensorflow.keras.optimizers import Adam # - Works\n\nfrom keras.losses import mean_squared_error, categorical_crossentropy, logcosh\nfrom keras.utils.np_utils import to_categorical\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:08:38.295009Z","iopub.execute_input":"2022-05-04T13:08:38.296684Z","iopub.status.idle":"2022-05-04T13:08:49.511752Z","shell.execute_reply.started":"2022-05-04T13:08:38.296589Z","shell.execute_reply":"2022-05-04T13:08:49.510207Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**Matrix Y contains the average grain yield, column 1: Grain yield for environment 1 and so on.**\n\n**Matrix X contains marker genotypes.**\n","metadata":{}},{"cell_type":"code","source":"# load data as a pandas dataframe\nX = pd.read_csv('/kaggle/input/genomicselection-data-weat/DATA/wheat.X', header=None, sep='\\s+')\nY = pd.read_csv('/kaggle/input/genomicselection-data-weat/DATA/wheat.Y', header=None, sep='\\s+')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:09:02.552588Z","iopub.execute_input":"2022-05-04T13:09:02.552923Z","iopub.status.idle":"2022-05-04T13:09:02.747496Z","shell.execute_reply.started":"2022-05-04T13:09:02.552889Z","shell.execute_reply":"2022-05-04T13:09:02.746471Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"X.head(10)\n#print('#'*50)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:09:05.276918Z","iopub.execute_input":"2022-05-04T13:09:05.278147Z","iopub.status.idle":"2022-05-04T13:09:05.314583Z","shell.execute_reply.started":"2022-05-04T13:09:05.278092Z","shell.execute_reply":"2022-05-04T13:09:05.313717Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print(X.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:09:20.071217Z","iopub.execute_input":"2022-05-04T13:09:20.071484Z","iopub.status.idle":"2022-05-04T13:09:20.077777Z","shell.execute_reply.started":"2022-05-04T13:09:20.071454Z","shell.execute_reply":"2022-05-04T13:09:20.076351Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(Y.head(10))\nprint('#'*50)\nprint(Y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:09:22.811239Z","iopub.execute_input":"2022-05-04T13:09:22.812005Z","iopub.status.idle":"2022-05-04T13:09:22.823104Z","shell.execute_reply.started":"2022-05-04T13:09:22.811960Z","shell.execute_reply":"2022-05-04T13:09:22.822237Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# data pattitioning into train and validation \nitrait=1\nX_train, X_test, y_train, y_test = train_test_split(X, Y[itrait], test_size=0.2)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:09:29.765988Z","iopub.execute_input":"2022-05-04T13:09:29.767196Z","iopub.status.idle":"2022-05-04T13:09:29.797328Z","shell.execute_reply.started":"2022-05-04T13:09:29.767068Z","shell.execute_reply":"2022-05-04T13:09:29.794727Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# print basic statistics: max, min, mean, sd\nprint('      min max mean sd')\nprint('Train: ', y_train.min(), y_train.max(), y_train.mean(), np.sqrt(y_train.var()))\nprint('Test: ', y_test.min(), y_test.max(), y_test.mean(), np.sqrt(y_test.var()))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:09:38.644376Z","iopub.execute_input":"2022-05-04T13:09:38.644691Z","iopub.status.idle":"2022-05-04T13:09:38.656248Z","shell.execute_reply.started":"2022-05-04T13:09:38.644658Z","shell.execute_reply":"2022-05-04T13:09:38.655029Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# basic histograms\nplt.title('train / test data')\nplt.hist(y_train, label='Train')\nplt.hist(y_test, label='Test')\nplt.legend(loc='best')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:09:42.063341Z","iopub.execute_input":"2022-05-04T13:09:42.063810Z","iopub.status.idle":"2022-05-04T13:09:42.377514Z","shell.execute_reply.started":"2022-05-04T13:09:42.063764Z","shell.execute_reply":"2022-05-04T13:09:42.376564Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**Marker PCA, use whole x with different color for train and test**","metadata":{}},{"cell_type":"code","source":"X = np.concatenate((X_train, X_test))\npca = PCA(n_components=2)\np = pca.fit(X).fit_transform(X)\nNtrain=X_train.shape[0]\nplt.title('PCA decomposition')\nplt.scatter(p[0:Ntrain,0], p[0:Ntrain,1], label='Train')\nplt.scatter(p[Ntrain:,0], p[Ntrain:,1], label='Test', color='orange')\nplt.legend(loc='best')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:10:02.504852Z","iopub.execute_input":"2022-05-04T13:10:02.506519Z","iopub.status.idle":"2022-05-04T13:10:03.102320Z","shell.execute_reply.started":"2022-05-04T13:10:02.506439Z","shell.execute_reply":"2022-05-04T13:10:03.101391Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**SNP preselection according to a simple GWAS**\n","metadata":{}},{"cell_type":"code","source":"pvals = []\nfor i in range(X_train.shape[1]):\n    b, intercept, r_value, p_value, std_err = stats.linregress(X_train[i], y_train)\n    pvals.append(-np.log10(p_value))\npvals = np.array(pvals)    \n\n# plot GWAS\nplt.ylabel('-log10 p-value')\nplt.xlabel('SNP')\nplt.plot(pvals, marker='o', color='red')\nplt.show()\n\n# select N_best most associated SNPs\n# N_best = X_train.shape[1] # all SNPs\nN_best = 100\nsnp_list = pvals.argsort()[-N_best:]\n\n# select by min P_value\nmin_p_value = 2 \nsnp_list = np.nonzero(pvals>min_p_value)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:10:42.078188Z","iopub.execute_input":"2022-05-04T13:10:42.078841Z","iopub.status.idle":"2022-05-04T13:10:42.799366Z","shell.execute_reply.started":"2022-05-04T13:10:42.078805Z","shell.execute_reply":"2022-05-04T13:10:42.798284Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"**Standard penalized methods**\n**lasso using scikit-learn**\n","metadata":{}},{"cell_type":"code","source":"# alpha is regularization parameter\nlasso = linear_model.Lasso(alpha=0.01)\nlasso.fit(X_train, y_train)\ny_hat = lasso.predict(X_test)\n\n# mean squared error\nmse = mean_squared_error(y_test, y_hat)\nprint('\\nMSE in prediction=', mse)\n\n# correlation btw predicted and observed\ncorr = np.corrcoef(y_test,y_hat)[0,1]\nprint('\\nCorr obs vs pred =', corr)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:10:57.844427Z","iopub.execute_input":"2022-05-04T13:10:57.844720Z","iopub.status.idle":"2022-05-04T13:10:58.039286Z","shell.execute_reply.started":"2022-05-04T13:10:57.844689Z","shell.execute_reply":"2022-05-04T13:10:58.038468Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\n\n# plot observed vs predicted targets\nplt.title('Lasso: observed vs predicted Y')\nplt.ylabel('Predicted')\nplt.xlabel('Observed')\nplt.scatter(y_test, y_hat, marker='o', cmap='viridis', alpha=0.3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:11:04.244793Z","iopub.execute_input":"2022-05-04T13:11:04.245124Z","iopub.status.idle":"2022-05-04T13:11:04.446427Z","shell.execute_reply.started":"2022-05-04T13:11:04.245089Z","shell.execute_reply":"2022-05-04T13:11:04.445213Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"**Implements a standard fully connected neural network for quantitative targets**","metadata":{}},{"cell_type":"code","source":"# number of SNPs in data\nnSNP = X_train.shape[1]\nnSNP","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:11:45.541913Z","iopub.execute_input":"2022-05-04T13:11:45.542867Z","iopub.status.idle":"2022-05-04T13:11:45.551773Z","shell.execute_reply.started":"2022-05-04T13:11:45.542803Z","shell.execute_reply":"2022-05-04T13:11:45.549690Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"**SoftPlus**\n\n**is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive**","metadata":{}},{"cell_type":"code","source":"# Instantiate\nmodel = Sequential()\n\n# add first layes\nmodel.add(Dense(64, input_dim=nSNP))\nmodel.add(Activation('relu'))\n# add second layer\nmodel.add(Dense(32))\nmodel.add(Activation('softplus'))\n# add third layer\nmodel.add(Dense(32))\nmodel.add(Activation('softplus'))\n#last, output layer\nmodel.add(Dense(1))\n\n# Model Compiling (https://keras.io/models/sequential/) \n# compile(optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None)\n# Stochastic Gradient Descent (‘sgd’) as optimization algorithm\n# Mean Squared Error as loss, ie, quantitative variable, regression\nmodel.compile(loss='mean_squared_error', optimizer='sgd')\n\n# list some properties\nmodel.summary()\n\n#tarining\n## fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1)\nmodel.fit(X_train, y_train, epochs=50)\n\n# cross-validation: get predicted target values\ny_hat = model.predict(X_test, batch_size=128)\n\nmse_prediction = model.evaluate(X_test, y_test, batch_size=128)\nprint('\\MSE in prediction = ', mse_prediction)\n\n# correlation btw predicted and observed\ncorr = np.corrcoef(y_test, y_hat[:,0])[0,1]\nprint('\\Corr obs vs pred =', corr)\n\n# plot observed vs predicted targets\nplt.title('MLP: observed vs predicetd Y')\nplt.ylabel('Predicted')\nplt.xlabel('Observed')\nplt.scatter(y_test, y_hat, marker='o')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:11:53.796174Z","iopub.execute_input":"2022-05-04T13:11:53.796494Z","iopub.status.idle":"2022-05-04T13:11:59.759631Z","shell.execute_reply.started":"2022-05-04T13:11:53.796460Z","shell.execute_reply":"2022-05-04T13:11:59.758870Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"**Controlling overfit: regularization, dropout and early stopping**","metadata":{}},{"cell_type":"code","source":"# deletes current model\ndel model\n\nmodel = Sequential()\n\n# Add l1 & l2 regularization in first layer\nmodel.add(Dense(64, input_dim=nSNP,\n                kernel_regularizer=regularizers.l2(0.01),\n                activity_regularizer=regularizers.l1(0.01)))\nmodel.add(Activation('relu'))\n# Add second layer\nmodel.add(Dense(32))\nmodel.add(Activation('softplus'))\n## Adding dropout to second layer\nmodel.add(Dropout(0.2))\n# Last, output layer\nmodel.add(Dense(1))\n\n# Model Compiling (https://keras.io/models/sequential/) \nmodel.compile(loss='mean_squared_error', optimizer='sgd')\n\n# Split the train set into proper train & validation\nX_train0, X_val, y_train0, y_val = train_test_split(X_train, y_train, test_size=0.1)\nnEpochs=100\n\n# Early stopping\nearly_stopper = EarlyStopping(monitor='val_loss', patience=10, min_delta=0.01)\nmodel.fit(X_train0, y_train0, epochs=nEpochs, verbose=1, validation_data=(X_val, y_val), callbacks=[early_stopper])\n\n# cross-validation\nmse_prediction = model.evaluate(X_test, y_test, batch_size=128)\nprint('\\nMSE in prediction =',mse_prediction)\n\n# correlation btw predicted and observed\ncorr = np.corrcoef(y_test, y_hat[:,0])[0,1]\nprint('\\Corr obs vs pred =', corr)\n\n# plot observed vs predicted targets\nplt.title('MLP: observed vs predicetd Y')\nplt.ylabel('Predicted')\nplt.xlabel('Observed')\nplt.scatter(y_test, y_hat, marker='o')\nplt.show()\n## In this case neither l1 nor l2 regularization helps","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:22:54.523724Z","iopub.execute_input":"2022-05-04T13:22:54.524060Z","iopub.status.idle":"2022-05-04T13:22:57.256679Z","shell.execute_reply.started":"2022-05-04T13:22:54.524022Z","shell.execute_reply":"2022-05-04T13:22:57.255221Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# talos items (for hyperparameter search)\n# import talos as ta\n# import wrangle as wr\n# from talos.metrics.keras_metrics import fmeasure_acc\n# from talos.model.layers import hidden_layers\n# from talos import live\n# from talos.model import lr_normalizer, early_stopper, hidden_layers\n","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:15:20.494068Z","iopub.execute_input":"2022-05-04T13:15:20.494479Z","iopub.status.idle":"2022-05-04T13:15:20.501219Z","shell.execute_reply.started":"2022-05-04T13:15:20.494445Z","shell.execute_reply":"2022-05-04T13:15:20.499016Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Defining pearson correlation as custom metric to model optimization in talos! \n# warning! you have to use acc in the metric name!\n\nfrom keras import backend as K\n\ndef acc_pearson_r(y_true, y_pred):\n    x = y_true\n    y = y_pred\n    mx = K.mean(x, axis=0)\n    my = K.mean(y, axis=0)\n    xm, ym = x - mx, y - my\n    r_num = K.sum(xm * ym)\n    x_square_sum = K.sum(xm * xm)\n    y_square_sum = K.sum(ym * ym)\n    r_den = K.sqrt(x_square_sum * y_square_sum)\n    r = r_num / r_den\n    return K.mean(r)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:14:20.014590Z","iopub.execute_input":"2022-05-04T13:14:20.014889Z","iopub.status.idle":"2022-05-04T13:14:20.026428Z","shell.execute_reply.started":"2022-05-04T13:14:20.014854Z","shell.execute_reply":"2022-05-04T13:14:20.025305Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Convolutional Neural Network","metadata":{}},{"cell_type":"code","source":"from keras.layers import Conv2D\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, LSTM\n","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:14:22.775730Z","iopub.execute_input":"2022-05-04T13:14:22.776881Z","iopub.status.idle":"2022-05-04T13:14:22.783601Z","shell.execute_reply.started":"2022-05-04T13:14:22.776782Z","shell.execute_reply":"2022-05-04T13:14:22.781946Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"nSNP = X_train.shape[1]\nnStride=3 # stride between convolutions\nnFilter=32 # no. of convolutions","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:14:25.176225Z","iopub.execute_input":"2022-05-04T13:14:25.176490Z","iopub.status.idle":"2022-05-04T13:14:25.181754Z","shell.execute_reply.started":"2022-05-04T13:14:25.176464Z","shell.execute_reply":"2022-05-04T13:14:25.180560Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Instantiate\nmodel_cnn = Sequential()\n\n# expand_dim is to match dimensions\nX2_train = np.expand_dims(X_train,axis=2)\nX2_test = np.expand_dims(X_test,axis=2)\n\n# add conolutional layer\nmodel_cnn.add(Conv1D(nFilter, kernel_size=3, strides=nStride, input_shape=(nSNP,1)))\n# add pooling layer: takes maximun of two consecutive values\nmodel_cnn.add(MaxPooling1D(pool_size=2))\n# solutions above are linearized to accommdate a standard layers\nmodel_cnn.add(Flatten())\nmodel_cnn.add(Dense(64))\nmodel_cnn.add(Activation('relu'))\nmodel_cnn.add(Dense(64))\nmodel_cnn.add(Activation('relu'))\nmodel_cnn.add(Dense(32))\nmodel_cnn.add(Activation('softplus'))\nmodel_cnn.add(Dense(1))\n\n# model compiling \nmodel_cnn.compile(loss='mean_squared_error', optimizer='sgd')\n\n# list some properties\nmodel_cnn.summary()\n\n# tarining\nmodel_cnn.fit(X2_train, y_train, epochs=100, verbose=0)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:21:36.969052Z","iopub.execute_input":"2022-05-04T13:21:36.969616Z","iopub.status.idle":"2022-05-04T13:21:54.123214Z","shell.execute_reply.started":"2022-05-04T13:21:36.969570Z","shell.execute_reply":"2022-05-04T13:21:54.120979Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# cross-validation \nmse_prediction = model_cnn.evaluate(X2_test, y_test, batch_size=128)\nprint('\\nMSE in prediction = ', mse_prediction)\n\n# get predicted target values\ny_hat = model_cnn.predict(X2_test, batch_size=128)\n\n# correlation btw predicted and observed\ncorr = np.corrcoef(y_test,y_hat[:,0])[0,1]\nprint('\\nCorr obs vs pred =',corr)\n\n\n# plot observed vs predicted targets\nplt.title('CNN: Observed vs Predicted Y')\nplt.ylabel('Predicted')\nplt.xlabel('Observed')\nplt.scatter(y_test, y_hat, marker='o', alpha=0.3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T13:22:10.014558Z","iopub.execute_input":"2022-05-04T13:22:10.014875Z","iopub.status.idle":"2022-05-04T13:22:10.720066Z","shell.execute_reply.started":"2022-05-04T13:22:10.014842Z","shell.execute_reply":"2022-05-04T13:22:10.718763Z"},"trusted":true},"execution_count":31,"outputs":[]}]}